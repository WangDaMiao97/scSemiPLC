{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:24:32.922859200Z",
     "start_time": "2025-11-09T02:24:31.231599500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.utils.data\n",
    "import torch.nn.parallel\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from model import actinn, clustering\n",
    "from model.utilities import *\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Loading\n",
    "Load single-cell data and cell type labels from 10x Genomics format, and align them based on cell names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:24:33.341863600Z",
     "start_time": "2025-11-09T02:24:32.917699500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 2781 × 23433\n",
      "    obs: 'batch', 'CellType'\n",
      "    var: 'gene_ids'\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "adata1 = sc.read_10x_mtx(\n",
    "    'data/Kidney/droplet/Kidney-10X_P4_5',  # the directory with the `.mtx` file\n",
    "    var_names='gene_symbols',  # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)\n",
    "adata1.obs_names = \"10X_P4_5_\" + adata1.obs_names\n",
    "adata2 = sc.read_10x_mtx(\n",
    "    'data/Kidney/droplet/Kidney-10X_P4_6',  # the directory with the `.mtx` file\n",
    "    var_names='gene_symbols',                # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)\n",
    "adata2.obs_names = \"10X_P4_6_\" + adata2.obs_names\n",
    "adata3 = sc.read_10x_mtx(\n",
    "    'data/Kidney/droplet/Kidney-10X_P7_5',  # the directory with the `.mtx` file\n",
    "    var_names='gene_symbols',                # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)\n",
    "adata3.obs_names = \"10X_P7_5_\" + adata3.obs_names\n",
    "adata = adata1.concatenate(adata2, adata3)\n",
    "split_names = adata.obs_names.str.split('-', expand=True)\n",
    "adata.obs_names = split_names.get_level_values(0)\n",
    "# label loading\n",
    "gt_df = pd.read_csv(\"data/Kidney/droplet/annotations_droplet.csv\")\n",
    "gt_df = gt_df[gt_df['tissue'] == 'Kidney']\n",
    "gt_df.index = gt_df[\"cell\"]\n",
    "# data and label mapping\n",
    "common_cells = adata.obs_names.intersection(gt_df.index)\n",
    "adata = adata[common_cells, :]\n",
    "gt_aligned = gt_df.loc[common_cells, :]\n",
    "adata.obs[\"CellType\"] = gt_aligned[\"cell_ontology_class\"]\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data preprocessing\n",
    "Apply standard QC, normalization, log transformation, and high-variability gene selection processes to prepare inputs for scSemiPLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:24:33.844487100Z",
     "start_time": "2025-11-09T02:24:33.344234200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 2781 × 1988\n",
      "    obs: 'batch', 'CellType', 'n_genes'\n",
      "    var: 'gene_ids', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n",
      "    uns: 'log1p', 'hvg'\n"
     ]
    }
   ],
   "source": [
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "sc.pp.normalize_total(adata, target_sum=1e4) ##标准化\n",
    "sc.pp.log1p(adata)\n",
    "adata.raw = adata\n",
    "sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "adata = adata[:, adata.var.highly_variable]\n",
    "sc.pp.scale(adata, max_value=10)\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training Function Definition\n",
    "Define the 'train' function, which integrates contrastive learning pre-training, supervised training, semi-supervised training, and annotation result evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:24:33.849720800Z",
     "start_time": "2025-11-09T02:24:33.844487100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(X_label, Y_label, X_unlabel, Y_unlabel, cluster, model, estimator, opt_est,\n",
    "          Pre_epochs=100, Supervised_epochs=200, SemiSupervised_epochs=150):\n",
    "\n",
    "    # Dataloader Definition\n",
    "    dataset = CustomDataset(data=X_unlabel, transform_args=transformation_list)\n",
    "    pretrain_loader = DataLoader(dataset, batch_size=512, shuffle=True, sampler=None,\n",
    "                                 batch_sampler=None, collate_fn=None, pin_memory=True)\n",
    "    # -------------------------------------- 1. Pretraing Stage -----------------------------------\n",
    "    # Initialize the annotation model using unlabeled data via contrastive loss.\n",
    "    opt_model = torch.optim.Adam(params=model_cla.parameters(), lr=5e-4, betas=(0.9, 0.999),\n",
    "                                     eps=1e-08, weight_decay=0.005, amsgrad=False)\n",
    "    for _ in tqdm(range(Pre_epochs), desc=\"Pre-Training\"):\n",
    "        model.train()\n",
    "        for batch_idx, (_, _, inputs_u_w, inputs_u_s) in enumerate(pretrain_loader):\n",
    "            inputs_u_w = inputs_u_w.to(device)\n",
    "            inputs_u_s = inputs_u_s.to(device)\n",
    "            N = inputs_u_w.shape[0]\n",
    "            opt_model.zero_grad()\n",
    "            _, logits_u_w = model(inputs_u_w)\n",
    "            _, logits_u_s = model(inputs_u_s)\n",
    "            Lc = ContrastiveLoss(logits_u_w, logits_u_s, 0.1)\n",
    "            Lc.backward()\n",
    "            opt_model.step()\n",
    "\n",
    "    # Establish a mapping relationship between the clusters obtained using the Hungarian algorithm and the cell types.\n",
    "    _, logits = model(torch.tensor(X_label).to(device))\n",
    "    pred_labels = logits.argmax(1)\n",
    "    gt_lables = torch.tensor(Y_label).to(device)\n",
    "    num_classes = max(pred_labels.max().item(), gt_lables.max().item()) + 1\n",
    "    matrix = torch.zeros((num_classes, num_classes), dtype=torch.long, device=pred_labels.device)\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            matrix[i, j] = torch.sum((pred_labels == i) & (gt_lables == j))\n",
    "    matrix_np = matrix.cpu().numpy()\n",
    "    row_ind, col_ind = linear_sum_assignment(-matrix_np)\n",
    "    gt2pred_mapping = {col_ind[i]: row_ind[i] for i in range(len(row_ind))}\n",
    "    pred2gt_mapping = {row_ind[i]: col_ind[i] for i in range(len(row_ind))}\n",
    "    aligned_gt_labels = gt_lables.clone()\n",
    "    for old_label, new_label in gt2pred_mapping.items():\n",
    "        aligned_gt_labels[gt_lables == old_label] = new_label # Application label mapping\n",
    "\n",
    "    # ------------------------------------ 2. Supervised Training Stage ---------------------------------\n",
    "    # Adjust the annotation model using aligned ground truth labels and data .\n",
    "    label_data = [[torch.tensor(feat), label, aligned_label] for feat, label, aligned_label in zip(X_label, Y_label, aligned_gt_labels.cpu())]\n",
    "    label_loader = DataLoader(label_data, batch_size=32, shuffle=True, sampler=None, batch_sampler=None, collate_fn=None, pin_memory=True)\n",
    "    opt_model = torch.optim.Adam(params=model_cla.parameters(), lr=1e-4, betas=(0.9, 0.999),\n",
    "                                     eps=1e-08, weight_decay=0.005, amsgrad=False)\n",
    "    for epoch in range(Supervised_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, _, aligned_targets) in enumerate(label_loader):\n",
    "            opt_model.zero_grad()\n",
    "            _, logits_x = model(inputs.to(device))\n",
    "            Ls = F.cross_entropy(logits_x, aligned_targets.to(device), reduction='mean')\n",
    "            Ls.backward()\n",
    "            opt_model.step()\n",
    "\n",
    "    # ---------------------------------- 3. Semi-Supervised Training Stage -------------------------------\n",
    "    # Integrate pseudo labels updated by clustering and consistency regularization to alternately optimize the annotation model and confidence estimator.\n",
    "    update_interval = 10\n",
    "    inputs_x = torch.tensor(X_label).to(device)\n",
    "    opt_model = torch.optim.Adam(params=model_cla.parameters(), lr=5e-5, betas=(0.9, 0.999),\n",
    "                                     eps=1e-08, weight_decay=0.005, amsgrad=False)\n",
    "    for epoch in tqdm(range(SemiSupervised_epochs), desc=\"SemiSupervised-Training\"):\n",
    "        if epoch % update_interval ==0:\n",
    "            with torch.no_grad():\n",
    "                # update_labels\n",
    "                model.eval()\n",
    "                init_target_centers = get_centers(net=model, data=inputs_x, labels=aligned_gt_labels,\n",
    "                                                  num_classes=NUM_CLASS)\n",
    "                cluster.set_init_centers(init_target_centers)\n",
    "                cluster.feature_clustering(model, data=torch.tensor(X_unlabel))\n",
    "                targets_sel = cluster.samples['p_label']\n",
    "\n",
    "        model.train()\n",
    "        estimator.train()\n",
    "        # Estimator optimization with model fixed\n",
    "        for batch_idx, (idx, input_u, inputs_u_w, inputs_u_s) in enumerate(pretrain_loader):\n",
    "            input_u = input_u.to(device)\n",
    "            inputs_u_w = inputs_u_w.to(device)\n",
    "            inputs_u_s = inputs_u_s.to(device)\n",
    "\n",
    "            opt_est.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                fea_sel, logits_sel = model(input_u)\n",
    "                _, logits_sel_s = model(inputs_u_s)\n",
    "                _, logits_sel_w = model(inputs_u_w)\n",
    "            input_s = torch.cat([fea_sel, logits_sel], dim=1)\n",
    "            input_s.requires_grad_(True)\n",
    "            sim_s = estimator(input_s)\n",
    "            # Consistency loss\n",
    "            Lcw = (F.cross_entropy(logits_sel_w, targets_sel[idx], reduction='none') * sim_s).mean()\n",
    "            Lcs = (F.cross_entropy(logits_sel_s, targets_sel[idx], reduction='none') * sim_s).mean()\n",
    "            Lu_est = Lcs + Lcw\n",
    "            Lu_est.backward()\n",
    "            opt_est.step()\n",
    "\n",
    "        # Model optimization with estimator fixed\n",
    "        for (idx, input_u, inputs_u_w, inputs_u_s) in pretrain_loader:\n",
    "            opt_model.zero_grad()\n",
    "            _, logits_x = model(inputs_x)\n",
    "            Ls = F.cross_entropy(logits_x, aligned_gt_labels, reduction='mean')  # 分类任务训练模型\n",
    "            input_u = input_u.to(device)\n",
    "            inputs_u_w = inputs_u_w.to(device)\n",
    "            inputs_u_s = inputs_u_s.to(device)\n",
    "            # opt_model.zero_grad()\n",
    "            fea_sel, logits_sel = model(input_u)\n",
    "            _, logits_sel_s = model(inputs_u_s)\n",
    "            _, logits_sel_w = model(inputs_u_w)\n",
    "            with torch.no_grad():\n",
    "                input_s = torch.cat([fea_sel.detach(), logits_sel.detach()], dim=1)\n",
    "                sim_s = estimator(input_s).detach()\n",
    "            Lcw = (F.cross_entropy(logits_sel_w, targets_sel[idx], reduction='none') * sim_s).mean()\n",
    "            Lcs = (F.cross_entropy(logits_sel_s, targets_sel[idx], reduction='none') * sim_s).mean()\n",
    "            Lu_model = Ls + 0.5 * (Lcs + Lcw)\n",
    "            Lu_model.backward()\n",
    "            opt_model.step()\n",
    "\n",
    "    # ---------------------------------- 4. Annotation Result Evaluation -------------------------------\n",
    "    model.eval()\n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    eval_fea = np.array([])\n",
    "    with torch.no_grad():\n",
    "        fea_x, output_x = model(torch.tensor(X_label).to(device))\n",
    "        _, predict_x = torch.max(output_x.squeeze(), 1)\n",
    "        y_true = np.append(y_true, Y_label)\n",
    "        y_pred = np.append(y_pred, predict_x.detach().cpu().numpy())\n",
    "        eval_fea = np.append(eval_fea, fea_x.detach().cpu().numpy())\n",
    "\n",
    "        fea_u, output_u = model(torch.tensor(X_unlabel).to(device))\n",
    "        _, predict_u = torch.max(output_u.squeeze(), 1)\n",
    "        y_true = np.append(y_true, Y_unlabel)\n",
    "        y_pred = np.append(y_pred, predict_u.detach().cpu().numpy())\n",
    "        eval_fea = np.append(eval_fea, fea_u.detach().cpu().numpy())\n",
    "\n",
    "    aligned_pred_labels = y_pred.copy()\n",
    "    # Mapping pred labels to ground truth's original numbers\n",
    "    for pred_label_idx, target_gt_idx in pred2gt_mapping.items():\n",
    "        aligned_pred_labels[y_pred == pred_label_idx] = target_gt_idx\n",
    "\n",
    "    accuracy = accuracy_score(y_true, aligned_pred_labels)\n",
    "    precision = precision_score(y_true, aligned_pred_labels, average=\"macro\")\n",
    "    recall = recall_score(y_true, aligned_pred_labels, average=\"macro\")\n",
    "    f1 = f1_score(y_true, aligned_pred_labels, average=\"macro\")\n",
    "\n",
    "    eval_fea = eval_fea.reshape(-1, 50)\n",
    "\n",
    "    return y_true, y_pred, eval_fea, 100.*accuracy, 100. * precision, 100. * recall, 100. * f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Main Program Execution\n",
    "Set the random seed, device, and data augmentation parameters, then train and evaluate the model through a stratified 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:46:15.837884800Z",
     "start_time": "2025-11-09T02:24:33.853986Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:33<00:00,  2.99it/s]\n",
      "SemiSupervised-Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 150/150 [01:35<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid F1:96.785%, valid_acc:99.173%\n",
      "valid pre:97.918%, valid_rec:96.115%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.13it/s]\n",
      "SemiSupervised-Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 150/150 [01:35<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid F1:96.778%, valid_acc:98.849%\n",
      "valid pre:96.121%, valid_rec:97.856%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.13it/s]\n",
      "SemiSupervised-Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 150/150 [01:35<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid F1:94.884%, valid_acc:98.670%\n",
      "valid pre:95.463%, valid_rec:95.261%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.13it/s]\n",
      "SemiSupervised-Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 150/150 [01:35<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid F1:97.754%, valid_acc:99.389%\n",
      "valid pre:98.010%, valid_rec:97.573%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.12it/s]\n",
      "SemiSupervised-Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 150/150 [01:35<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid F1:96.592%, valid_acc:99.065%\n",
      "valid pre:97.248%, valid_rec:96.200%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.13it/s]\n",
      "SemiSupervised-Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 150/150 [01:35<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid F1:96.455%, valid_acc:99.029%\n",
      "valid pre:98.532%, valid_rec:95.070%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]\n",
      "SemiSupervised-Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 150/150 [01:35<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid F1:95.593%, valid_acc:98.346%\n",
      "valid pre:94.415%, valid_rec:97.589%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.11it/s]\n",
      "SemiSupervised-Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 150/150 [01:34<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid F1:96.957%, valid_acc:98.993%\n",
      "valid pre:97.910%, valid_rec:96.251%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.13it/s]\n",
      "SemiSupervised-Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 150/150 [01:35<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid F1:98.404%, valid_acc:99.425%\n",
      "valid pre:99.089%, valid_rec:97.760%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.13it/s]\n",
      "SemiSupervised-Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 150/150 [01:35<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid F1:97.821%, valid_acc:99.245%\n",
      "valid pre:97.411%, valid_rec:98.335%\n",
      "\n",
      "average accuracy:99.018%, average F1:96.802%\n",
      "average precision:97.212%, average recall:96.801%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Record the results of the metrics\n",
    "valid_f1_sum, valid_acc_sum = 0, 0\n",
    "valid_pre_sum, valid_rec_sum = 0, 0\n",
    "out_pred = pd.DataFrame()\n",
    "out_true = pd.DataFrame()\n",
    "out_batch = pd.DataFrame()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cluster = clustering.Clustering(0.005, 128 * 9, device=device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Data Preparation\n",
    "dataset_name = \"Kidney\"\n",
    "type_to_label_dict = {'kidney capillary endothelial cell': 0, 'kidney cell': 1,\n",
    "                      'kidney collecting duct epithelial cell': 2,\n",
    "                      'kidney loop of Henle ascending limb epithelial cell': 3,\n",
    "                      'kidney proximal straight tubule epithelial cell': 4, 'leukocyte': 5, 'macrophage': 6,\n",
    "                      'mesangial cell': 7}\n",
    "\n",
    "# Data Augmentation Settings\n",
    "transformation_list = [{  # weak\n",
    "    'mask_percentage': 0.5, 'apply_mask_prob': 0.8,\n",
    "    'noise_percentage': 0.5, 'sigma': 0.5, 'apply_noise_prob': 0.0\n",
    "}, {  # strong\n",
    "    'mask_percentage': 0.5, 'apply_mask_prob': 0.0,\n",
    "    'noise_percentage': 0.5, 'sigma': 0.5, 'apply_noise_prob': 0.8\n",
    "}]\n",
    "\n",
    "X = np.array(adata.X).astype(np.float32)\n",
    "Y = convert_type2label(adata.obs[\"CellType\"], type_to_label_dict)\n",
    "feature_size = X.shape[1]\n",
    "NUM_CLASS = len(type_to_label_dict)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "index = 0\n",
    "label_num = list(range(NUM_CLASS))\n",
    "for unlabel_index, label_index in skf.split(X, Y):\n",
    "    X_label, X_unlabel = X[label_index], X[unlabel_index]\n",
    "    Y_label, Y_unlabel = Y[label_index], Y[unlabel_index]\n",
    "\n",
    "    # Model definition\n",
    "    model_cla = actinn.ACTINN(output_dim=NUM_CLASS, input_size=feature_size).to(device)\n",
    "    model_cla.apply(init_weights)\n",
    "    # Confidence estimator\n",
    "    model_est = actinn.Con_estimator(NUM_CLASS=NUM_CLASS).to(device)\n",
    "    model_est.apply(init_weights)\n",
    "    optimizer_est = torch.optim.Adam(params=model_est.parameters(), lr=5e-4, betas=(0.9, 0.999),\n",
    "                                     eps=1e-08, weight_decay=0.005, amsgrad=False)\n",
    "    # Model Training\n",
    "    label_true, pred, fea, val_acc, val_pre, val_rec, val_f1 = \\\n",
    "        train(X_label, Y_label, X_unlabel, Y_unlabel, cluster=cluster,\n",
    "              model=model_cla, estimator=model_est, opt_est=optimizer_est)\n",
    "    end_time = time.time()\n",
    "    print('valid F1:{:.3f}%, valid_acc:{:.3f}%'.format(val_f1, val_acc))\n",
    "    print('valid pre:{:.3f}%, valid_rec:{:.3f}%\\n'.format(val_pre, val_rec))\n",
    "    # Results recording\n",
    "    valid_f1_sum += val_f1\n",
    "    valid_acc_sum += val_acc\n",
    "    valid_pre_sum += val_pre\n",
    "    valid_rec_sum += val_rec\n",
    "\n",
    "    index += 1\n",
    "\n",
    "print('average accuracy:{:.3f}%, average F1:{:.3f}%'.format(valid_acc_sum / 10, valid_f1_sum / 10))\n",
    "print('average precision:{:.3f}%, average recall:{:.3f}%\\n'.format(valid_pre_sum / 10, valid_rec_sum / 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
